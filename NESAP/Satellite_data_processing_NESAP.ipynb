{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f58c2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Tue Oct 27 20:19:13 2020\\n\\n@author: Brandon McNabb (bmcnabb@eoas.ubc.ca)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Tue Oct 27 20:19:13 2020\n",
    "\n",
    "@author: Brandon McNabb (bmcnabb@eoas.ubc.ca)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14187e-1279-46c6-a5b6-6fcb77baba82",
   "metadata": {},
   "source": [
    "## A script for post-processing data used in \"NESAP_Sulfur_Climatology_v5.ipynb\". Data files were obtained from the following sources:\n",
    "---\n",
    "### PMEL (DMS 1997-2017)\n",
    " - https://saga.pmel.noaa.gov/dms/\n",
    "\n",
    "---\n",
    "### Aqua MODIS (2002-2017) & SeaWiFS/Aqua TERRA (1997-2002)\n",
    " \n",
    "#### SSHA (Sea Surface Height Anomalies)\n",
    " - https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1812_2019010412.nc.html\n",
    " - https://podaac.jpl.nasa.gov/dataset/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812\n",
    "\n",
    "#### Chlorophyll-a, Calcite (PIC), photosyntheically active radiation (daily=PAR, instantaneous=iPAR), sea surface temperature (SST), fluorescence line height (nFLH), diffuse attenuation coefficients (kd)\n",
    " - https://oceancolor.gsfc.nasa.gov/l3/\n",
    "---\n",
    "### Copernicus wind speeds (2007-2017)\n",
    " - https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=WIND_GLO_PHY_CLIMATE_L4_REP_012_003\n",
    "---\n",
    "### NPP (VGPM model)\n",
    "##### From Aqua MODIS (2002-2017):\n",
    " - http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.m.chl.m.sst.php\n",
    "\n",
    "##### From SeaWiFS (1997-2002):\n",
    " - http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.s.chl.a.sst.php\n",
    "---\n",
    "### World Ocean Atlas (WOA18) (sea surface nitrate SSN)\n",
    " - https://www.ncei.noaa.gov/access/world-ocean-atlas-2018/\n",
    "---\n",
    "### MIMOC Climatology (sea surface salinity (SSS), mixed layer depth (MLD)):\n",
    " - https://www.pmel.noaa.gov/mimoc/\n",
    "---\n",
    "### ETOPO2 Bathymetry\n",
    " - https://rda.ucar.edu/datasets/ds759.3/\n",
    "---\n",
    "### NCP Algorithm:\n",
    "Li, Z., & Cassar, N. (2016). Satellite estimates of net community production based on O 2 /Ar observations and comparison to other estimates. Global Biogeochemical Cycles, 30(5), 735–752. https://doi.org/10/f8v6bh\n",
    "\n",
    "---\n",
    "### NPQ-corrected Fluorescence Yield Algorithm:\n",
    "Behrenfeld, M. J., Westberry, T. K., Boss, E. S., O’Malley, R. T., Siegel, D. A., Wiggert, J. D., Franz, B. A., McClain, C. R., Feldman, G. C., Doney, S. C., Moore, J. K., Dall’Olmo, G., Milligan, A. J., Lima, I., & Mahowald, N. (2009). Satellite-detected ﬂuorescence reveals global physiology of ocean phytoplankton. Biogeosciences, 6, 16. https://doi.org/10/fdn3f2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811a004-f306-4e9a-bbdb-4e889cab00e5",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95418d6",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Start timer"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "runtime_start = timeit.default_timer() # start the clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ece71",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Import packages"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import os\n",
    "from dateutil import parser\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from pyhdf.SD import SD, SDC\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002f1bc-c7dd-42e9-bd66-e338bf94e020",
   "metadata": {},
   "source": [
    "## Define constants and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18270894",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Set constants & parameters"
   },
   "outputs": [],
   "source": [
    "\n",
    "#~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "# *** Make sure lat/lons bounds are the same as analysis script!! ***\n",
    "#~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "\n",
    "#### Spatial grid resolution (degrees):\n",
    "grid = 0.25\n",
    "\n",
    "#### Define lat/lon constraints\n",
    "\n",
    "# Line P/La Perouse area:\n",
    "# Set directory to write data files to:\n",
    "# write_dir = 'C:/Users/bcamc/OneDrive/Desktop/Python/Projects/py_eosc510/Final Project/NEPac_Data/'\n",
    "# min_lat = 43\n",
    "# max_lat = 60\n",
    "# min_lon = -147\n",
    "# max_lon = -122\n",
    "\n",
    "# Full NESAP region:\n",
    "# Set directory to write data files to:\n",
    "# write_dir = 'C:/Users/bcamc/OneDrive/Desktop/Python/Projects/py_eosc510/Final Project/NESAP_Data/'\n",
    "# write_dir = 'C:/Users/bcamc/OneDrive/Desktop/Python/Projects/py_eosc510/Final Project/NESAP_Data_v2/'\n",
    "write_dir = 'C:/Users/bcamc/OneDrive/Desktop/Python/Projects/py_eosc510/Final Project/NESAP_Data_res/'\n",
    "\n",
    "min_lat = 40\n",
    "max_lat = 61\n",
    "min_lon = -180\n",
    "max_lon = -122\n",
    "\n",
    "#### Define bins\n",
    "latbins = np.arange(min_lat,max_lat+grid,grid)\n",
    "lonbins = np.arange(min_lon,max_lon+grid,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1733bb9-e2b6-4553-8ec1-55a180b81aa1",
   "metadata": {},
   "source": [
    "## Extract dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859dee44",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Find dimensions of MODIS data"
   },
   "outputs": [],
   "source": [
    "\n",
    "start = timeit.default_timer() # start the clock\n",
    "#-----------------------------------------------------------------------------\n",
    "# Set path:\n",
    "# file directory shape:\n",
    "# -/MODIS\n",
    "#       |-/Data\n",
    "#           |-/1_Chl\n",
    "#               |-/*.nc\n",
    "#           |-/2_PIC\n",
    "#               |-/*.nc\n",
    "#           |-/3_SST\n",
    "#               |-/*.nc\n",
    "#           |-/4_PAR\n",
    "#               |-/*.nc\n",
    "#           |-/5_Kd\n",
    "#               |-/*.nc\n",
    "#           |-/6_POC\n",
    "#               |-/*.nc\n",
    "#           |-/7_FLH\n",
    "#               |-/*.nc\n",
    "#           |-/8_SSHA\n",
    "#               |-/*.nc\n",
    "#           |-/9_NPP\n",
    "#               |-/*.hdf\n",
    "#           |-/10_MIMOC_MLD\n",
    "#               |-/*.hdf\n",
    "#           |-/11_MIMOC_TS\n",
    "#               |-/*.hdf\n",
    "\n",
    "directory = 'E:/Satellite/Data_Subset_NESAP' # E: or C:, depending if using hard drive/thumb drive\n",
    "folders = os.listdir(directory) # calls specific datasets grouped in folders\n",
    "num_folders = 14 # set this to the number of variables\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print('finding dimensions...')\n",
    "\n",
    "# ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "# preallocate variable:\n",
    "dims = np.empty((num_folders,4))\n",
    "# ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "for count, folder in enumerate(folders):\n",
    "    path = os.path.join(directory, folder)\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for n, file in enumerate(files):\n",
    "            if count < 9 and n == 1:\n",
    "                # if count <= 4:\n",
    "                # find lat/lon dimensions:\n",
    "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "                # Open up one of the datasets\n",
    "                filepath = subdir + os.sep + file\n",
    "                check = xr.open_dataset(filepath)\n",
    "                # Pull the lat/lon from it:\n",
    "                lat = check.coords['lat'].values\n",
    "                lon = check.coords['lon'].values\n",
    "                # First find the indices to index by lat/lon\n",
    "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                # Need these as pandas dataframes to using binning scheme:\n",
    "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
    "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
    "                data_long = pd.DataFrame(data=d)\n",
    "                # Bin data as averages across gridded spatial bins:\n",
    "                to_bin = lambda x: np.round(x / grid) * grid\n",
    "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                # Rename binned columns + drop mean lat/lons:\n",
    "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "            if count == 9 and n == 1: # Data configured differently for SSHA data\n",
    "                # if count <= 4:\n",
    "                # find lat/lon dimensions:\n",
    "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "                # Open up one of the datasets\n",
    "                filepath = subdir + os.sep + file\n",
    "                check = xr.open_dataset(filepath)\n",
    "                # Pull the lat/lon from it:\n",
    "                lat = check.coords['Latitude'].values\n",
    "                lon = check.coords['Longitude'].values\n",
    "                # First find the indices to index by lat/lon\n",
    "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                # Need these as pandas dataframes to using binning scheme:\n",
    "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
    "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
    "                data_long = pd.DataFrame(data=d)\n",
    "                # Bin data as averages across gridded spatial bins:\n",
    "                to_bin = lambda x: np.round(x / grid) * grid\n",
    "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                # Rename binned columns + drop mean lat/lons:\n",
    "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "            if count == 10 and n == 1: # Data configured differently for SSH data\n",
    "                # find lat/lon dimensions:\n",
    "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "                # Open up one of the datasets\n",
    "                filepath_SSH = subdir + os.sep + file\n",
    "                check = xr.open_dataset(filepath_SSH)\n",
    "                # lets pull the lat/lon from it:\n",
    "                lat = check.coords['latitude'].values\n",
    "                lon = check.coords['longitude'].values-180 # convert to W/E coords\n",
    "                # First find the indices to index by lat/lon\n",
    "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                # Need these as pandas dataframes to using binning scheme:\n",
    "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
    "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
    "                data_long = pd.DataFrame(data=d)\n",
    "                # Bin data as averages across gridded spatial bins:\n",
    "                to_bin = lambda x: np.round(x / grid) * grid\n",
    "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                # Rename binned columns + drop mean lat/lons:\n",
    "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "            if count == 11 and n == 1: # Data configured differently for NPP data\n",
    "                # find lat/lon dimensions:\n",
    "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "                # Open up one of the datasets\n",
    "                filepath_NPP = subdir + os.sep + file\n",
    "                vars_ = SD(filepath_NPP, SDC.READ)\n",
    "                # data = np.flipud(vars_.select('npp').get())\n",
    "                # lets pull the lat/lon from it:\n",
    "                n = 180/1080\n",
    "                lat = np.arange(-90,90,n)\n",
    "                lon = np.arange(-180,180,n)\n",
    "                # First find the indices to index by lat/lon\n",
    "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                # Need these as pandas dataframes to using binning scheme:\n",
    "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
    "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
    "                data_long = pd.DataFrame(data=d)\n",
    "                # Bin data as averages across gridded spatial bins:\n",
    "                to_bin = lambda x: np.round(x / grid) * grid\n",
    "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                # Rename binned columns + drop mean lat/lons:\n",
    "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "            if count == 12 or count == 11 and n == 1: # Data configured differently for MLD data\n",
    "                # find lat/lon dimensions:\n",
    "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "                # Open up one of the datasets\n",
    "                filepath = subdir + os.sep + file\n",
    "                check = xr.open_dataset(filepath)\n",
    "                # lets pull the lat/lon from it:\n",
    "                lat = check.LATITUDE.values\n",
    "                lon = check.LONGITUDE.values-180 # convert to W/E coords\n",
    "                # First find the indices to index by lat/lon\n",
    "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                # Need these as pandas dataframes to using binning scheme:\n",
    "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
    "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
    "                data_long = pd.DataFrame(data=d)\n",
    "                # Bin data as averages across gridded spatial bins:\n",
    "                to_bin = lambda x: np.round(x / grid) * grid\n",
    "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                # Rename binned columns + drop mean lat/lons:\n",
    "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "            else:\n",
    "                pass\n",
    "    # extract number of files (datasets) per folder + lat/lon dimensions:\n",
    "    dims[count] = np.array([int(count), int(np.size(files)), int(data_proc.shape[0]*np.size(files)), int(data_proc.shape[1])])\n",
    "    print('\\nData length:',str(data_proc.shape[0]))\n",
    "    print('In folder ' + '\"' + folder + '\":')\n",
    "    print('Number of datasets = '+ str(np.size(files)))\n",
    "end = timeit.default_timer() # stop the clock\n",
    "print('\\n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~')\n",
    "print('Execution time:')\n",
    "print(str(round(end-start,5)),'secs')\n",
    "print(str(round((end-start)/60,5)),'mins')\n",
    "print(str(round((end-start)/3600,5)),'hrs')\n",
    "del check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f443d20-891a-4cfa-ac7a-435fd0326c01",
   "metadata": {},
   "source": [
    "## Pre-allocate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b959c",
   "metadata": {
    "title": "Pre-allocate variables"
   },
   "outputs": [],
   "source": [
    "# Disk space can't handle computing all the files at once (>30 gb worth),\n",
    "# so use Dask package to parallel compute over CPUs\n",
    "#-----------------------------------------------------------------------------\n",
    "# Pre-allocate extraction variables as a dict using Dask:\n",
    "variables = {'chl':dd.from_array(da.empty((dims[0,2],dims[0,3]))),\n",
    "             'PIC':dd.from_array(da.empty((dims[1,2],dims[1,3]))),\n",
    "             'SST':dd.from_array(da.empty((dims[2,2],dims[2,3]))),\n",
    "             'PAR':dd.from_array(da.empty((dims[3,2],dims[3,3]))),\n",
    "             'Kd':dd.from_array(da.empty((dims[4,2],dims[4,3]))),\n",
    "             'POC':dd.from_array(da.empty((dims[5,2],dims[5,3]))),\n",
    "             'FLH':dd.from_array(da.empty((dims[6,2],dims[6,3]))),\n",
    "             'iPAR':dd.from_array(da.empty((dims[7,2],dims[7,3]))),\n",
    "             'CDOM':dd.from_array(da.empty((dims[8,2],dims[8,3]))),\n",
    "             'SSHA':dd.from_array(da.empty((dims[9,2],dims[9,3]))),\n",
    "             'SSH':dd.from_array(da.empty((dims[10,2],dims[10,3]))),\n",
    "             'NPP':dd.from_array(da.empty((dims[11,2],dims[11,3]))),\n",
    "             'MLD':dd.from_array(da.empty((dims[12,2],dims[12,3]))),\n",
    "             'SAL':dd.from_array(da.empty((dims[13,2],dims[13,3])))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f812892-1380-4b24-a23b-7b6a9e768fbc",
   "metadata": {},
   "source": [
    "## Extract MODIS/SeaWiFS/MIMOC from file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef45c7",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Extracting MODIS/SeaWiFS/MIMOC data from file directory"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print('Beginning extraction loop...')\n",
    "start = timeit.default_timer() # start the clock\n",
    "for count, (k, folder) in enumerate(zip(variables, folders)):\n",
    "    print('\\nAccessing ' + '\"' + folder + '\"' + ' folder...')\n",
    "    print()\n",
    "    path = os.path.join(directory, folder)\n",
    "    if count == 0 or count==10: # My PC is RAM limited so this line bypasses looping at higher res (0.25x0.25) below to extract one variable at a time, saving memory space; if computing resources are not limited, comment out to extract all variables at once\n",
    "        for subdir, dirs, files in os.walk(path):\n",
    "            for n, file in enumerate(files):\n",
    "                filepath = subdir + os.sep + file\n",
    "                # Print status: starting iteration\n",
    "                print(folder + ' (' + str(count+1) + ' of ' + str(np.size(folders)) + ' folders)' + ': Extracting... ' + str(n+1) +' of ' + str(np.size(files)))\n",
    "                #-----------------------------------------------------------------\n",
    "                # Extract data\n",
    "                if count < 12:\n",
    "                    if count <= 9:\n",
    "                        # Open netCDF file\n",
    "                        vars_ = xr.open_dataset(filepath)\n",
    "                        if count == 0:\n",
    "                            # Chl:\n",
    "                            data = vars_.chlor_a.values\n",
    "                        if count == 1:\n",
    "                            # PIC:\n",
    "                            data = vars_.pic.values\n",
    "                        if count == 2:\n",
    "                            # SST:\n",
    "                            data = vars_.sst.values\n",
    "                        if count == 3:\n",
    "                            # PAR:\n",
    "                            data = vars_.par.values\n",
    "                        if count == 4:\n",
    "                            # Kd (490 nm)\n",
    "                            data = vars_.Kd_490.values\n",
    "                        if count == 5:\n",
    "                            # POC\n",
    "                            data = vars_.poc.values\n",
    "                        if count == 6:\n",
    "                            # FLH\n",
    "                            data = vars_.nflh.values\n",
    "                        if count == 7:\n",
    "                            # iPAR\n",
    "                            data = vars_.ipar.values\n",
    "                        if count == 8:\n",
    "                            # CDOM\n",
    "                            data = vars_.adg_443_giop.values\n",
    "                        lat = vars_.coords['lat'].values\n",
    "                        lon = vars_.coords['lon'].values\n",
    "                        time = pd.to_datetime(vars_.time_coverage_start)+datetime.timedelta(days=2) # this adds 2 days to correct for SeaWiFS files starting on the last day of previous month\n",
    "                    if count == 9:\n",
    "                        # SSHA\n",
    "                        # Open netCDF file\n",
    "                        vars_ = xr.open_dataset(filepath)\n",
    "                        data = vars_.SLA.values[0,:,:].T # transpose to arrange as lat x lon\n",
    "                        lat = vars_.coords['Latitude'].values\n",
    "                        lon = vars_.coords['Longitude'].values\n",
    "                        lon = pd.Series(lon).where(lon<180, lon-360).values # convert to W/E coords\n",
    "                        time = pd.to_datetime(vars_.coords['Time'].values)\n",
    "                    if count == 10:\n",
    "                        # SSH\n",
    "                        # Open netCDF file\n",
    "                        vars_ = xr.open_dataset(filepath)\n",
    "                        data = vars_.SSH.values[0,:,:] # transpose to arrange as lat x lon\n",
    "                        lat = vars_.latitude.values\n",
    "                        lon = vars_.longitude.values\n",
    "                        time = pd.to_datetime(vars_.time_coverage_start)+datetime.timedelta(days=2) # this adds 2 days to correct for SeaWiFS files starting on the last day of previous month\n",
    "                    if count == 11:\n",
    "                        # NPP\n",
    "                        vars_ = SD(filepath, SDC.READ)\n",
    "                        data = np.flipud(vars_.select('npp').get())\n",
    "                        ndeg = 180/1080\n",
    "                        lat = np.arange(-90,90,ndeg)\n",
    "                        lon = np.arange(-180,180,ndeg)\n",
    "                        # extract date from julian day in filename:\n",
    "                        julian_date = filepath.split(\".\")[1]\n",
    "                        time = pd.to_datetime(datetime.datetime.strptime(julian_date[2:], '%y%j').date()) #strftime requires year to be cropped to last 2 digits\n",
    "                    #-----------------------------------------------------------------\n",
    "                    # case 1: match pixels to high resolution satellite data\n",
    "                    if lon[1]-lon[0] >= grid: #i.e. if downsampling to finer grid size\n",
    "                        # Regrid data and interpolate though:\n",
    "                        # First find the indices to index by lat/lon\n",
    "                        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                        \n",
    "                        # Restrict the data to the NE Pacific lat/lons\n",
    "                        data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "                        \n",
    "                        # Create data matrix with existing coordinates\n",
    "                        data_mat = pd.DataFrame(data_indexed)\n",
    "                        data_mat.columns = lon[loninds].flatten()\n",
    "                        data_mat.index = lat[latinds].flatten()\n",
    "                        \n",
    "                        # Create a new matrix with new gridded coordinates\n",
    "                        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
    "                        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
    "                        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
    "                        new_shape.columns=lon_new\n",
    "                        new_shape.index=lat_new\n",
    "                        \n",
    "                        # Now reindex exisitng data to new coordinates + add in NaNs to interpolate through later\n",
    "                        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
    "                        data_proc = data_proc.rename(columns={0:'data'})\n",
    "                        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
    "                        data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
    "                        data_proc.reset_index(inplace=True)\n",
    "                        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
    "                        data_proc.reset_index(inplace=True)\n",
    "                        \n",
    "                    #-----------------------------------------------------------------\n",
    "                    # case 2: downsample to courser grid\n",
    "                    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
    "                        # Bin the data\n",
    "                        \n",
    "                        # First find the indices to index by lat/lon\n",
    "                        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                        \n",
    "                        # Restrict the data to the NE Pacific lat/lons\n",
    "                        data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "                        \n",
    "                        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                        time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
    "                        \n",
    "                        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                        # Need these as pandas dataframes to using binning scheme:\n",
    "                        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
    "                        data_long = pd.DataFrame(data=d)\n",
    "                        \n",
    "                        # Bin data as averages across gridded spatial bins:\n",
    "                        to_bin = lambda x: np.round(x / grid) * grid\n",
    "                        data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                        \n",
    "                        # Rename binned columns + drop mean lat/lons:\n",
    "                        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                        data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "                    # #-----------------------------------------------------------------\n",
    "                    # # Bin the data\n",
    "                    \n",
    "                    # # First find the indices to index by lat/lon\n",
    "                    # latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                    # loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                    \n",
    "                    # # Restrict the data to the NE Pacific lat/lons\n",
    "                    # data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "                    \n",
    "                    # # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                    # lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                    # lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                    # time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
    "                    \n",
    "                    # # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                    # # Need these as pandas dataframes to use binning scheme:\n",
    "                    # d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
    "                    # data_long = pd.DataFrame(data=d)\n",
    "                    \n",
    "                    # # Bin data as averages across gridded spatial bins:\n",
    "                    # to_bin = lambda x: np.round(x / grid) * grid\n",
    "                    # data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                    # data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                    # data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                    \n",
    "                    # # Rename binned columns + drop mean lat/lons:\n",
    "                    # data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                    # data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "                #-----------------------------------------------------------------\n",
    "                # Lets extract MIMOC data...\n",
    "                if count >= 12:\n",
    "                    if count == 12:\n",
    "                        # Open netCDF file\n",
    "                        vars_ = xr.open_dataset(filepath)\n",
    "                        data = vars_.DEPTH_MIXED_LAYER.values\n",
    "                    if count == 13:\n",
    "                        # Open netCDF file\n",
    "                        vars_ = xr.open_dataset(filepath)\n",
    "                        data = vars_.SALINITY[0,:,:].values # select values at surface layer (0 m)\n",
    "                    lat = vars_.LATITUDE.values\n",
    "                    lon = vars_.LONGITUDE.values\n",
    "                    lon = pd.Series(lon).where(lon<180, lon-360).values # convert to W/E coords\n",
    "                    julian_date = file.split(\"month\")[1].split('.nc')[0]\n",
    "                    time = pd.to_datetime(datetime.datetime.strptime(julian_date[:2], '%m').date()) #strftime requires year to be cropped to last 2 digits\n",
    "                    #-----------------------------------------------------------------\n",
    "                    # case 1: match pixels to high resolution satellite data\n",
    "                    if lon[1]-lon[0] >= grid: #i.e. if downsampling to finer grid size\n",
    "                        # Regrid data and interpolate though:\n",
    "                        # First find the indices to index by lat/lon\n",
    "                        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                        \n",
    "                        # Restrict the data to the NE Pacific lat/lons\n",
    "                        data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "                        \n",
    "                        # Create data matrix with existing coordinates\n",
    "                        data_mat = pd.DataFrame(data_indexed)\n",
    "                        data_mat.columns = lon[loninds].flatten()\n",
    "                        data_mat.index = lat[latinds].flatten()\n",
    "                        \n",
    "                        # Create a new matrix with new gridded coordinates\n",
    "                        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
    "                        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
    "                        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
    "                        new_shape.columns=lon_new\n",
    "                        new_shape.index=lat_new\n",
    "                        \n",
    "                        # Now reindex exisitng data to new coordinates + add in NaNs to interpolate through later\n",
    "                        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
    "                        data_proc = data_proc.rename(columns={0:'data'})\n",
    "                        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
    "                        data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
    "                        data_proc.reset_index(inplace=True)\n",
    "                        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
    "                        data_proc.reset_index(inplace=True)\n",
    "                        \n",
    "                    #-----------------------------------------------------------------\n",
    "                    # case 2: downsample to courser grid\n",
    "                    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
    "                        # Bin the data\n",
    "                        \n",
    "                        # First find the indices to index by lat/lon\n",
    "                        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "                        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "                        \n",
    "                        # Restrict the data to the NE Pacific lat/lons\n",
    "                        data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "                        \n",
    "                        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "                        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "                        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "                        time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
    "                        \n",
    "                        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "                        # Need these as pandas dataframes to using binning scheme:\n",
    "                        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
    "                        data_long = pd.DataFrame(data=d)\n",
    "                        \n",
    "                        # Bin data as averages across gridded spatial bins:\n",
    "                        to_bin = lambda x: np.round(x / grid) * grid\n",
    "                        data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "                        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "                        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "                        \n",
    "                        # Rename binned columns + drop mean lat/lons:\n",
    "                        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "                        data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "                            \n",
    "                # Save to output array\n",
    "                if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
    "                    variables[k] = dd.from_pandas(data_proc, chunksize=10)\n",
    "                else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
    "                    variables[k] = dd.concat([variables[k],dd.from_pandas(data_proc, chunksize=10)])\n",
    "                 \n",
    "                # Print status: completed iteration\n",
    "                print(folder + ' (' + str(count+1) + ' of ' + str(np.size(folders)) + ' folders)' + ': Completed ' + str(n+1) + ' of ' + str(np.size(files)))\n",
    "        print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print('Ta-da! ' + str(count+1) + ' of ' + str(np.size(folders)) + ' datasets processed')\n",
    "end = timeit.default_timer() # stop the clock\n",
    "print('\\n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~')\n",
    "print('Execution time:') # output computation times:\n",
    "print(str(round(end-start,5)),'secs')\n",
    "print(str(round((end-start)/60,5)),'mins')\n",
    "print(str(round((end-start)/3600,5)),'hrs')\n",
    "del vars_, data_long, data, data_indexed, data_proc, time_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa791f4-4b22-43b9-8b9c-cbe9aa71f0e7",
   "metadata": {},
   "source": [
    "## Extracting Copernicus Wind Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc3a65",
   "metadata": {
    "title": "Extracting COPERNICUS wind field data (singular netCDF; 0.25x0.25)"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nExtracting Copernicus wind field data...')\n",
    "print('\\nLoading data...')\n",
    "# Pre-allocate dask array to store new binned data to\n",
    "wind = dd.from_array(da.empty((1,6)))\n",
    "# Load up wind data file (Copernicus timeseries data is downloaded into a single netCDF)\n",
    "windfile = xr.open_dataset('E:/Satellite/CERSAT-GLO-REP_WIND_L4-OBS_FULL_TIME_SERIE_1607246388558.nc')\n",
    "\n",
    "# Define variables\n",
    "# shape is (time,depth,lat,lon)\n",
    "lat = windfile.coords['latitude'].values\n",
    "lon = windfile.coords['longitude'].values\n",
    "U = windfile.eastward_wind.values[:,0,:,:]\n",
    "V = windfile.northward_wind.values[:,0,:,:]\n",
    "wspd = windfile.wind_speed.values[:,0,:,:]\n",
    "timerange = pd.to_datetime(windfile.time.values,utc=True).strftime('%Y%m').to_series().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Define indices\n",
    "latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "PMEL = pd.read_csv('C:/Users/bcamc/OneDrive/Desktop/Python/Projects/py_eosc510/Final Project/NEPac_Data/PMEL_Herr_all_data.csv')\n",
    "timeinds = np.unique(pd.to_datetime(PMEL['DateTime']).dt.strftime('%Y%m'))[1:]\n",
    "\n",
    "idx = np.arange(0,timerange.size,1)\n",
    "times = pd.DataFrame(idx,index=timerange)\n",
    "# 1 indexes from '200706' onwards\n",
    "timepts = times.loc[timeinds[1:]].values\n",
    "\n",
    "\n",
    "print('\\nBegin extraction loop...')\n",
    "# Loop to bin/reshape\n",
    "for n,pt in enumerate(timepts[:,0]):\n",
    "    print(str(n+1)+' of '+str(np.size(timepts[:,0]))+' iterations...')\n",
    "    # index data\n",
    "    wspd_indexed = wspd[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
    "    U_indexed = U[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
    "    V_indexed = V[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
    "    \n",
    "    # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "    lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "    lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "    time_mat = np.tile(timerange[pt], len(np.ravel(lat_mat)))\n",
    "    \n",
    "    # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "    # Need these as pandas dataframes to using binning scheme:\n",
    "    d = {'datetime':time_mat,\n",
    "         'lat':np.ravel(lat_mat),\n",
    "         'lon':np.ravel(lon_mat),\n",
    "         'wspd':np.ravel(wspd_indexed),\n",
    "         'U':np.ravel(U_indexed),\n",
    "         'V':np.ravel(V_indexed)}\n",
    "    data_long = pd.DataFrame(data=d)\n",
    "    \n",
    "    # Since data is already at 0.25x0.25 scale, \"bin\" by shifting coords by 0.125 to match other variable coordinates\n",
    "    data_long.loc[:,'lat'] = data_long.loc[:,'lat']+0.125\n",
    "    data_long.loc[:,'lon'] = data_long.loc[:,'lon']+0.125\n",
    "                \n",
    "    # Save to output array\n",
    "    if n == 0:\n",
    "        wind = dd.from_pandas(data_long, chunksize=10)\n",
    "    else:\n",
    "        wind = dd.concat([wind,dd.from_pandas(data_long, chunksize=10)])\n",
    "    print(str(n+1)+' of '+str(np.size(timepts[:,0]))+' iterations completed!')\n",
    "del windfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26251b-a431-45b8-bb72-4c0328bf2b4d",
   "metadata": {},
   "source": [
    "## Processing WOA18 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66de69",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Processing WOA18 data (csv; 1x1 to 0.25x0.25)"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nProcessing WOA18 SSN data...')\n",
    "print('\\nLoading data...')\n",
    "#------------------------------------------------------------------------------\n",
    "#### Nitrate\n",
    "#------------------------------------------------------------------------------\n",
    "# Pre-allocate dask array to store new binned data to\n",
    "SSN = dd.from_array(da.empty((1,3)))\n",
    "\n",
    "for n,pt in enumerate(np.array([6,7,8])):\n",
    "    # Load up wind data file (Copernicus timeseries data is downloaded into a single netCDF)\n",
    "    data = pd.read_csv('E:/woa18_all_n0'+str(pt)+'an01.csv', header=[1]).reset_index().drop('index', axis=1).rename(columns={'#COMMA SEPARATED LATITUDE':'lat',' LONGITUDE':'lon', ' AND VALUES AT DEPTHS (M):0':0}).set_index(['lat','lon']).loc[:,0]\n",
    "    lon = data.index.get_level_values('lon')\n",
    "    lat = data.index.get_level_values('lat')\n",
    "    data = data.unstack()\n",
    "    time = pd.to_datetime({'year': [2000],'month': [pt],'day': [1]})\n",
    "    \n",
    "    # case 1: match pixels to high resolution satellite data\n",
    "    if lon[1]-lon[0] >= grid: #i.e. if downsampling to finer grid size\n",
    "        # Regrid data and interpolate though:\n",
    "        # First find the indices to index by lat/lon\n",
    "        # latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "        # loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "        \n",
    "        # Restrict the data to the NE Pacific lat/lons\n",
    "        data_indexed = data.loc[min_lat:max_lat, min_lon:max_lon]\n",
    "        \n",
    "        # Create data matrix with existing coordinates\n",
    "        data_mat = pd.DataFrame(data_indexed)\n",
    "        # data_mat.columns = lon[min_lon:max_lon]\n",
    "        # data_mat.index = lat[min_lat:max_lat]\n",
    "        \n",
    "        # Create a new matrix with new gridded coordinates\n",
    "        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
    "        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
    "        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
    "        new_shape.columns=lon_new\n",
    "        new_shape.index=lat_new\n",
    "        \n",
    "        # Now reindex exisitng data to new coordinates + add in NaNs to interpolate through later\n",
    "        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
    "        data_proc = data_proc.rename(columns={0:'data'})\n",
    "        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
    "        data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
    "        data_proc.reset_index(inplace=True)\n",
    "        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
    "        data_proc.reset_index(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "    # case 2: downsample to courser grid\n",
    "    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
    "        # Bin the data\n",
    "        \n",
    "        # First find the indices to index by lat/lon\n",
    "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "        \n",
    "        # Restrict the data to the NE Pacific lat/lons\n",
    "        data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "        \n",
    "        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "        time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
    "        \n",
    "        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "        # Need these as pandas dataframes to using binning scheme:\n",
    "        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
    "        data_long = pd.DataFrame(data=d)\n",
    "        \n",
    "        # Bin data as averages across gridded spatial bins:\n",
    "        to_bin = lambda x: np.round(x / grid) * grid\n",
    "        data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "        \n",
    "        # Rename binned columns + drop mean lat/lons:\n",
    "        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "        data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "\n",
    "    # Save to output array\n",
    "    if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
    "        SSN = dd.from_pandas(data_proc, chunksize=10)\n",
    "    else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
    "        SSN = dd.concat([SSN,dd.from_pandas(data_proc, chunksize=10)])\n",
    "    print(str(n+1)+' of '+str(np.size(np.array([6,7,8])))+' iterations completed!')\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#### Phosphate\n",
    "#------------------------------------------------------------------------------\n",
    "# Pre-allocate dask array to store new binned data to\n",
    "SSP = dd.from_array(da.empty((1,3)))\n",
    "\n",
    "for n,pt in enumerate(np.array([6,7,8])):\n",
    "    # Load up wind data file (Copernicus timeseries data is downloaded into a single netCDF)\n",
    "    data = pd.read_csv('E:/woa18_all_p0'+str(pt)+'an01.csv', header=[1]).reset_index().drop('index', axis=1).rename(columns={'#COMMA SEPARATED LATITUDE':'lat',' LONGITUDE':'lon', ' AND VALUES AT DEPTHS (M):0':0}).set_index(['lat','lon']).loc[:,0]\n",
    "    lon = data.index.get_level_values('lon')\n",
    "    lat = data.index.get_level_values('lat')\n",
    "    data = data.unstack()\n",
    "    time = pd.to_datetime({'year': [2000],'month': [pt],'day': [1]})\n",
    "    \n",
    "    # case 1: match pixels to high resolution satellite data\n",
    "    if lon[1]-lon[0] >= grid: #i.e. if downsampling to finer grid size\n",
    "        # Regrid data and interpolate though:\n",
    "        # First find the indices to index by lat/lon\n",
    "        # latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "        # loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "        \n",
    "        # Restrict the data to the NE Pacific lat/lons\n",
    "        data_indexed = data.loc[min_lat:max_lat, min_lon:max_lon]\n",
    "        \n",
    "        # Create data matrix with existing coordinates\n",
    "        data_mat = pd.DataFrame(data_indexed)\n",
    "        # data_mat.columns = lon[min_lon:max_lon]\n",
    "        # data_mat.index = lat[min_lat:max_lat]\n",
    "        \n",
    "        # Create a new matrix with new gridded coordinates\n",
    "        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
    "        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
    "        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
    "        new_shape.columns=lon_new\n",
    "        new_shape.index=lat_new\n",
    "        \n",
    "        # Now reindex exisitng data to new coordinates + add in NaNs to interpolate through later\n",
    "        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
    "        data_proc = data_proc.rename(columns={0:'data'})\n",
    "        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
    "        data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
    "        data_proc.reset_index(inplace=True)\n",
    "        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
    "        data_proc.reset_index(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "    # case 2: downsample to courser grid\n",
    "    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
    "        # Bin the data\n",
    "        \n",
    "        # First find the indices to index by lat/lon\n",
    "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "        \n",
    "        # Restrict the data to the NE Pacific lat/lons\n",
    "        data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "        \n",
    "        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "        time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
    "        \n",
    "        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "        # Need these as pandas dataframes to using binning scheme:\n",
    "        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
    "        data_long = pd.DataFrame(data=d)\n",
    "        \n",
    "        # Bin data as averages across gridded spatial bins:\n",
    "        to_bin = lambda x: np.round(x / grid) * grid\n",
    "        data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "        \n",
    "        # Rename binned columns + drop mean lat/lons:\n",
    "        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "        data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "\n",
    "    # Save to output array\n",
    "    if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
    "        SSP = dd.from_pandas(data_proc, chunksize=10)\n",
    "    else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
    "        SSP = dd.concat([SSP,dd.from_pandas(data_proc, chunksize=10)])\n",
    "    print(str(n+1)+' of '+str(np.size(np.array([6,7,8])))+' iterations completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8016a-cbe3-4496-8654-6da9c7e6e85d",
   "metadata": {},
   "source": [
    "## ETOPO2 Bathymetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a6edd",
   "metadata": {
    "title": "ETOPO2 data"
   },
   "outputs": [],
   "source": [
    "# This bathymetry data is used to mask out land pixels when interpolating\n",
    "vars_ = xr.open_dataset('E:/etopo2.nc')\n",
    "data = vars_.btdata.values\n",
    "lat = vars_.coords['lat'].values\n",
    "lon = vars_.coords['lon'].values\n",
    "#-----------------------------------------------------------------\n",
    "# Bin the data\n",
    "\n",
    "# First find the indices to index by lat/lon\n",
    "latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "\n",
    "# Restrict the data to the NE Pacific lat/lons\n",
    "data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "\n",
    "# Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
    "\n",
    "# Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "# Need these as pandas dataframes to using binning scheme:\n",
    "d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
    "data_long = pd.DataFrame(data=d)\n",
    "\n",
    "# Bin data as averages across gridded spatial bins:\n",
    "to_bin = lambda x: np.round(x / grid) * grid\n",
    "data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
    "\n",
    "# Rename binned columns + drop mean lat/lons:\n",
    "data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "data_proc.reset_index(inplace=True) # remove index specification on columns#-----------------------------------------------------------------\n",
    "# Bin the data\n",
    "\n",
    "# First find the indices to index by lat/lon\n",
    "latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
    "loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
    "\n",
    "# Restrict the data to the NE Pacific lat/lons\n",
    "data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
    "\n",
    "# Convert time/lats/lons into repeating matrices to match data dimensions\n",
    "lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
    "lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
    "time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
    "\n",
    "# Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
    "# Need these as pandas dataframes to using binning scheme:\n",
    "d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
    "data_long = pd.DataFrame(data=d)\n",
    "\n",
    "# Bin data as averages across gridded spatial bins:\n",
    "to_bin = lambda x: np.round(x / grid) * grid\n",
    "data_long['latbins'] = data_long.lat.map(to_bin)\n",
    "data_long['lonbins'] = data_long.lon.map(to_bin)\n",
    "data_proc = data_long.groupby(['datetime', 'lonbins', 'latbins']).mean()\n",
    "\n",
    "# Rename binned columns + drop mean lat/lons:\n",
    "data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
    "data_proc.reset_index(inplace=True) # remove index specification on columns\n",
    "etopo = data_proc.iloc[:,1:]\n",
    "etopo.set_index(['lonbins','latbins'], inplace=True)\n",
    "del data_proc, vars_, data_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f44efd-862f-4681-a388-8a3b1e8a4696",
   "metadata": {},
   "source": [
    "## Reshape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1c324-7852-47b8-b53e-ff076395a00d",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Reshape satellite data"
   },
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# NOTE: If extracting one variable at a time above, then code below needs to run\n",
    "# line-by-line for that specific variable (for both reshaping + interpolating)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#### Reshape data\n",
    "# Pivot to move lat/lon pairs to columns\n",
    "# reset_index pulls the dates back into a column\n",
    "print('Reshaping satellite data...')\n",
    "CHL_sat = variables['chl'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "CHL_sat = CHL_sat.groupby(CHL_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['chl']\n",
    "print('CHL done')\n",
    "SSH_sat = variables['SSH'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "SSH_sat = SSH_sat.groupby(SSH_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['SSHA']\n",
    "print('SSH done')\n",
    "SSHA_sat = variables['SSHA'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "SSHA_sat = SSHA_sat.groupby(SSHA_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['SSHA']\n",
    "print('SSHA done')\n",
    "SST_sat = variables['SST'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "SST_sat = SST_sat.groupby(SST_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['SST']\n",
    "print('SST done')\n",
    "PIC_sat = variables['PIC'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "PIC_sat = PIC_sat.groupby(PIC_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['PIC']\n",
    "print('PIC done')\n",
    "PAR_sat = variables['PAR'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "PAR_sat = PAR_sat.groupby(PAR_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['PAR']\n",
    "print('PAR done')\n",
    "Kd_sat = variables['Kd'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "Kd_sat = Kd_sat.groupby(Kd_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['Kd']\n",
    "print('Kd done')\n",
    "POC_sat = variables['POC'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "POC_sat = POC_sat.groupby(POC_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['POC']\n",
    "print('POC done')\n",
    "FLH_sat = variables['FLH'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "FLH_sat = FLH_sat.groupby(FLH_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['POC']\n",
    "print('FLH done')\n",
    "iPAR_sat = variables['iPAR'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "iPAR_sat = iPAR_sat.groupby(iPAR_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['POC']\n",
    "print('iPAR done')\n",
    "CDOM_sat = variables['CDOM'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "CDOM_sat = CDOM_sat.groupby(CDOM_sat['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['POC']\n",
    "print('CDOM done')\n",
    "SSN_woa = SSN.compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "SSN_woa = SSN_woa.groupby(SSN_woa['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['POC']\n",
    "print('SSN done')\n",
    "SSP_woa = SSP.compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "SSP_woa = SSP_woa.groupby(SSP_woa['datetime'].dt.strftime('%m')).mean()\n",
    "# del variables['POC']\n",
    "print('SSP done')\n",
    "NPP_sat = variables['NPP'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "NPP_sat = NPP_sat.groupby(NPP_sat['datetime'].dt.strftime('%m')).mean()\n",
    "NPP_sat = NPP_sat.replace(-9999,'NaN').astype(float)\n",
    "NPP_sat = NPP_sat.reindex_like(CHL_sat)\n",
    "# del variables['NPP']\n",
    "print('NPP done')\n",
    "MLD = variables['MLD'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "MLD['datetime'] = MLD['datetime'].dt.strftime('%m')\n",
    "MLD.set_index('datetime', inplace=True)\n",
    "# del variables['MLD']\n",
    "print('MLD done')\n",
    "SAL_argo = variables['SAL'].compute().pivot(index='datetime',columns=['latbins','lonbins'], values='data').reset_index()\n",
    "SAL_argo['datetime'] = SAL_argo['datetime'].dt.strftime('%m')\n",
    "SAL_argo.set_index('datetime', inplace=True)\n",
    "# del variables['SAL']\n",
    "print('SAL done')\n",
    "WSPD_sat = wind.compute().pivot(index='datetime',columns=['lat','lon'], values='wspd').reset_index()\n",
    "WSPD_sat['datetime'] = pd.to_datetime(WSPD_sat['datetime'], format='%Y%m')\n",
    "WSPD_sat = WSPD_sat.groupby(WSPD_sat['datetime'].dt.strftime('%m')).mean()\n",
    "WSPD_sat = WSPD_sat.reindex_like(CHL_sat)\n",
    "print('WSPD done')\n",
    "U_sat = wind.compute().pivot(index='datetime',columns=['lat','lon'], values='U').reset_index()\n",
    "U_sat['datetime'] = pd.to_datetime(U_sat['datetime'], format='%Y%m')\n",
    "U_sat = U_sat.groupby(U_sat['datetime'].dt.strftime('%m')).mean()\n",
    "U_sat = U_sat.reindex_like(CHL_sat)\n",
    "print('U done')\n",
    "V_sat = wind.compute().pivot(index='datetime',columns=['lat','lon'], values='V').reset_index()\n",
    "V_sat['datetime'] = pd.to_datetime(V_sat['datetime'], format='%Y%m')\n",
    "V_sat = V_sat.groupby(V_sat['datetime'].dt.strftime('%m')).mean()\n",
    "V_sat = V_sat.reindex_like(CHL_sat)\n",
    "print('V done')\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2ed66-4c48-4fc7-99ac-5fe355dec6c1",
   "metadata": {},
   "source": [
    "## Interpolate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02545ccd-7ca0-41ff-ac2b-50e61bb3f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Interpolate through the NaNs\n",
    "print()\n",
    "print('Interpolating data...')\n",
    "func='linear'\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### Chl:\n",
    "    \n",
    "# stack to column vectors:\n",
    "CHL_stack = CHL_sat.stack(dropna=False).stack(dropna=False)\n",
    "CHL_stack_dropna = CHL_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = CHL_stack_dropna.index\n",
    "ind_to_interp = CHL_stack.index\n",
    "d = CHL_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in CHL_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in CHL_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "CHL_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'CHL':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "CHL_sat_interp = CHL_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='CHL').reindex_like(CHL_sat)\n",
    "\n",
    "# Restrict chlorophyll to limit of detection:\n",
    "# CHL_sat_interp[CHL_sat_interp<0.01]=0.01\n",
    "print('Chlorophyll done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### PIC:\n",
    "    \n",
    "# stack to column vectors:\n",
    "PIC_stack = PIC_sat.stack(dropna=False).stack(dropna=False)\n",
    "PIC_stack_dropna = PIC_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = PIC_stack_dropna.index\n",
    "ind_to_interp = PIC_stack.index\n",
    "d = PIC_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in PIC_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in PIC_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "PIC_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'PIC':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "PIC_sat_interp = PIC_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='PIC').reindex_like(CHL_sat)\n",
    "\n",
    "print('PIC done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### PAR:\n",
    "    \n",
    "# stack to column vectors:\n",
    "PAR_stack = PAR_sat.stack(dropna=False).stack(dropna=False)\n",
    "PAR_stack_dropna = PAR_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = PAR_stack_dropna.index\n",
    "ind_to_interp = PAR_stack.index\n",
    "d = PAR_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in PAR_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in PAR_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "PAR_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'PAR':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "PAR_sat_interp = PAR_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='PAR').reindex_like(CHL_sat)\n",
    "# PAR_sat_interp[PAR_sat_interp<0]=0\n",
    "print('PAR done')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### SSH:\n",
    "    \n",
    "# stack to column vectors:\n",
    "SSH_stack = SSH_sat.stack(dropna=False).stack(dropna=False)\n",
    "SSH_stack_dropna = SSH_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = SSH_stack_dropna.index\n",
    "ind_to_interp = SSH_stack.index\n",
    "d = SSH_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in SSH_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in SSH_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "SSH_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'SSH':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "SSH_sat_interp = SSH_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='SSH').reindex_like(CHL_sat)\n",
    "\n",
    "print('SSH done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### SSHA:\n",
    "    \n",
    "# stack to column vectors:\n",
    "SSHA_stack = SSHA_sat.stack(dropna=False).stack(dropna=False)\n",
    "SSHA_stack_dropna = SSHA_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = SSHA_stack_dropna.index\n",
    "ind_to_interp = SSHA_stack.index\n",
    "d = SSHA_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in SSHA_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in SSHA_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "SSHA_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'SSHA':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "SSHA_sat_interp = SSHA_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='SSHA').reindex_like(CHL_sat)\n",
    "\n",
    "print('SSHA done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### SST:\n",
    "    \n",
    "# stack to column vectors:\n",
    "SST_stack = SST_sat.stack(dropna=False).stack(dropna=False)\n",
    "SST_stack_dropna = SST_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = SST_stack_dropna.index\n",
    "ind_to_interp = SST_stack.index\n",
    "d = SST_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in SST_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in SST_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "SST_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'SST':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "SST_sat_interp = SST_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='SST').reindex_like(CHL_sat)\n",
    "print('SST done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### Kd:\n",
    "    \n",
    "# stack to column vectors:\n",
    "Kd_stack = Kd_sat.stack(dropna=False).stack(dropna=False)\n",
    "Kd_stack_dropna = Kd_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = Kd_stack_dropna.index\n",
    "ind_to_interp = Kd_stack.index\n",
    "d = Kd_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in Kd_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in Kd_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "Kd_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'Kd':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "Kd_sat_interp = Kd_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='Kd').reindex_like(CHL_sat)\n",
    "\n",
    "print('Kd done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### POC:\n",
    "    \n",
    "# stack to column vectors:\n",
    "POC_stack = POC_sat.stack(dropna=False).stack(dropna=False)\n",
    "POC_stack_dropna = POC_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = POC_stack_dropna.index\n",
    "ind_to_interp = POC_stack.index\n",
    "d = POC_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in POC_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in POC_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "POC_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'POC':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "POC_sat_interp = POC_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='POC').reindex_like(CHL_sat)\n",
    "\n",
    "print('POC done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### FLH:\n",
    "    \n",
    "# stack to column vectors:\n",
    "FLH_stack = FLH_sat.stack(dropna=False).stack(dropna=False)\n",
    "FLH_stack_dropna = FLH_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = FLH_stack_dropna.index\n",
    "ind_to_interp = FLH_stack.index\n",
    "d = FLH_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in FLH_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in FLH_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "FLH_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'FLH':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "FLH_sat_interp = FLH_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='FLH').reindex_like(CHL_sat)\n",
    "\n",
    "print('FLH done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### iPAR:\n",
    "    \n",
    "# stack to column vectors:\n",
    "iPAR_stack = iPAR_sat.stack(dropna=False).stack(dropna=False)\n",
    "iPAR_stack_dropna = iPAR_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = iPAR_stack_dropna.index\n",
    "ind_to_interp = iPAR_stack.index\n",
    "d = iPAR_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in iPAR_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in iPAR_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "iPAR_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'iPAR':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "iPAR_sat_interp = iPAR_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='iPAR').reindex_like(CHL_sat)\n",
    "\n",
    "print('iPAR done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### CDOM:\n",
    "    \n",
    "# stack to column vectors:\n",
    "CDOM_stack = CDOM_sat.stack(dropna=False).stack(dropna=False)\n",
    "CDOM_stack_dropna = CDOM_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = CDOM_stack_dropna.index\n",
    "ind_to_interp = CDOM_stack.index\n",
    "d = CDOM_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in CDOM_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in CDOM_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "CDOM_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'CDOM':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "CDOM_sat_interp = CDOM_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='CDOM').reindex_like(CHL_sat)\n",
    "\n",
    "print('CDOM done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### SSN:\n",
    "    \n",
    "# stack to column vectors:\n",
    "SSN_stack = SSN_woa.stack(dropna=False).stack(dropna=False)\n",
    "SSN_stack_dropna = SSN_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = SSN_stack_dropna.index\n",
    "ind_to_interp = SSN_stack.index\n",
    "d = SSN_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in SSN_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in SSN_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "SSN_woa_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'SSN':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "SSN_woa_interp = SSN_woa_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='SSN').reindex_like(CHL_sat)\n",
    "\n",
    "print('SSN done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### SSP:\n",
    "    \n",
    "# stack to column vectors:\n",
    "SSP_stack = SSP_woa.stack(dropna=False).stack(dropna=False)\n",
    "SSP_stack_dropna = SSP_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = SSP_stack_dropna.index\n",
    "ind_to_interp = SSP_stack.index\n",
    "d = SSP_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in SSP_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in SSP_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "SSP_woa_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'SSP':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "SSP_woa_interp = SSP_woa_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='SSP').reindex_like(CHL_sat)\n",
    "\n",
    "print('SSP done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### NPP:\n",
    "    \n",
    "# stack to column vectors:\n",
    "NPP_stack = NPP_sat.stack(dropna=False).stack(dropna=False)\n",
    "NPP_stack_dropna = NPP_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = NPP_stack_dropna.index\n",
    "ind_to_interp = NPP_stack.index\n",
    "d = NPP_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in NPP_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in NPP_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "NPP_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'NPP':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "NPP_sat_interp = NPP_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='NPP').reindex_like(CHL_sat)\n",
    "print('NPP done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### MLD:\n",
    "    \n",
    "# stack to column vectors:\n",
    "MLD_stack = MLD.stack(dropna=False).stack(dropna=False)\n",
    "MLD_stack_dropna = MLD_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = MLD_stack_dropna.index\n",
    "ind_to_interp = MLD_stack.index\n",
    "d = MLD_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in MLD_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in MLD_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "MLD_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'MLD':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "MLD_interp = MLD_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='MLD').reindex_like(CHL_sat)\n",
    "\n",
    "print('MLD done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### Salinity:\n",
    "    \n",
    "# stack to column vectors:\n",
    "SAL_argo_stack = SAL_argo.stack(dropna=False).stack(dropna=False)\n",
    "SAL_argo_stack_dropna = SAL_argo_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = SAL_argo_stack_dropna.index\n",
    "ind_to_interp = SAL_argo_stack.index\n",
    "d = SAL_argo_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in SAL_argo_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in SAL_argo_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "SAL_argo_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'SAL':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "SAL_argo_interp = SAL_argo_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='SAL').reindex_like(CHL_sat)\n",
    "\n",
    "print('SAL done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### WSPD:\n",
    "    \n",
    "# stack to column vectors:\n",
    "WSPD_stack = WSPD_sat.stack(dropna=False).stack(dropna=False)\n",
    "WSPD_stack_dropna = WSPD_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = WSPD_stack_dropna.index\n",
    "ind_to_interp = WSPD_stack.index\n",
    "d = WSPD_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in WSPD_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in WSPD_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "WSPD_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'WSPD':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "WSPD_sat_interp = WSPD_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='WSPD').reindex_like(CHL_sat)\n",
    "WSPD_sat_interp[WSPD_sat_interp<0]=0\n",
    "print('WSPD done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### U:\n",
    "    \n",
    "# stack to column vectors:\n",
    "U_stack = U_sat.stack(dropna=False).stack(dropna=False)\n",
    "U_stack_dropna = U_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = U_stack_dropna.index\n",
    "ind_to_interp = U_stack.index\n",
    "d = U_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in U_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in U_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "U_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'U':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "U_sat_interp = U_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='U').reindex_like(CHL_sat)\n",
    "print('U done')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#### V:\n",
    "    \n",
    "# stack to column vectors:\n",
    "V_stack = V_sat.stack(dropna=False).stack(dropna=False)\n",
    "V_stack_dropna = V_stack.dropna()\n",
    "\n",
    "# Get index and values to build interpolation function\n",
    "ind_from_raw = V_stack_dropna.index\n",
    "ind_to_interp = V_stack.index\n",
    "d = V_stack_dropna.values\n",
    "d = da.from_array(d, chunks=(d.shape[0]))\n",
    "\n",
    "date_ind = np.asarray([x[0] for x in V_stack.index]).astype(float)\n",
    "date_ind_raw = np.asarray([x[0] for x in V_stack_dropna.index]).astype(float)\n",
    "\n",
    "# Build RBF interpolation functions:\n",
    "rbfinterp6 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 6],\n",
    "                                  d[date_ind_raw == 6],function='linear')\n",
    "\n",
    "rbfinterp7 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 7],\n",
    "                                  d[date_ind_raw == 7],function='linear')\n",
    "\n",
    "rbfinterp8 = scipy.interpolate.Rbf(da.array([n[0] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[1] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  da.array([n[2] for n in ind_from_raw])[date_ind_raw == 8],\n",
    "                                  d[date_ind_raw == 8],function='linear')\n",
    "\n",
    "# Interpolate values:\n",
    "V_sat_interp = pd.DataFrame({'DateTime':np.array([n[0] for n in ind_to_interp]),\n",
    "                            'Lon':np.array([n[1] for n in ind_to_interp]),\n",
    "                            'Lat':np.array([n[2] for n in ind_to_interp]),\n",
    "                            'V':np.concatenate([rbfinterp6(np.array([n[0] for n in ind_to_interp])[date_ind == 6],np.array([n[1] for n in ind_to_interp])[date_ind == 6],np.array([n[2] for n in ind_to_interp])[date_ind == 6]),\n",
    "                                             rbfinterp7(np.array([n[0] for n in ind_to_interp])[date_ind == 7],np.array([n[1] for n in ind_to_interp])[date_ind == 7],np.array([n[2] for n in ind_to_interp])[date_ind == 7]),\n",
    "                                             rbfinterp8(np.array([n[0] for n in ind_to_interp])[date_ind == 8],np.array([n[1] for n in ind_to_interp])[date_ind == 8],np.array([n[2] for n in ind_to_interp])[date_ind == 8])],axis=0)})\n",
    "\n",
    "\n",
    "# Reshape and filter out negative interpolated values\n",
    "V_sat_interp = V_sat_interp.pivot(index='DateTime',\n",
    "                                      columns=['Lat','Lon'],\n",
    "                                      values='V').reindex_like(CHL_sat)\n",
    "print('V done')\n",
    "#-----------------------------------------------------------------------------\n",
    "# Clear some space in the memory\n",
    "del ind_from_raw, ind_to_interp, d, date_ind, date_ind_raw, rbfinterp6, rbfinterp7, rbfinterp8\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfc901-9cf8-4539-b9fc-1cad9fb39522",
   "metadata": {},
   "source": [
    "## Write processed data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2d792-c3a9-47d2-a824-5155d42fa1c2",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Write data to files"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Write files\n",
    "CHL_sat.to_csv(write_dir+'CHL_sat_'+str(grid)+'.csv') # for reshaping\n",
    "\n",
    "etopo.to_csv(write_dir+'etopo2_'+str(grid)+'.csv')\n",
    "\n",
    "CHL_sat_interp.to_csv(write_dir+'CHL_sat_interp_'+str(grid)+'.csv')\n",
    "PAR_sat_interp.to_csv(write_dir+'PAR_sat_interp_'+str(grid)+'.csv')\n",
    "PIC_sat_interp.to_csv(write_dir+'PIC_sat_interp_'+str(grid)+'.csv')\n",
    "SSHA_sat_interp.to_csv(write_dir+'SSHA_sat_interp_'+str(grid)+'.csv')\n",
    "SSH_sat_interp.to_csv(write_dir+'SSH_sat_interp_'+str(grid)+'.csv')\n",
    "SST_sat_interp.to_csv(write_dir+'SST_sat_interp_'+str(grid)+'.csv')\n",
    "Kd_sat_interp.to_csv(write_dir+'Kd_sat_interp_'+str(grid)+'.csv')\n",
    "POC_sat_interp.to_csv(write_dir+'POC_sat_interp_'+str(grid)+'.csv')\n",
    "FLH_sat_interp.to_csv(write_dir+'FLH_sat_interp_'+str(grid)+'.csv')\n",
    "iPAR_sat_interp.to_csv(write_dir+'iPAR_sat_interp_'+str(grid)+'.csv')\n",
    "CDOM_sat_interp.to_csv(write_dir+'CDOM_sat_interp_'+str(grid)+'.csv')\n",
    "SSN_woa_interp.to_csv(write_dir+'SSN_woa_interp_'+str(grid)+'.csv')\n",
    "SSP_woa_interp.to_csv(write_dir+'SSP_woa_interp_'+str(grid)+'.csv')\n",
    "NPP_sat_interp.to_csv(write_dir+'NPP_sat_interp_'+str(grid)+'.csv')\n",
    "MLD_interp.to_csv(write_dir+'MLD_argo_interp_'+str(grid)+'.csv')\n",
    "SAL_argo_interp.to_csv(write_dir+'SAL_argo_interp_'+str(grid)+'.csv')\n",
    "WSPD_sat_interp.to_csv(write_dir+'WSPD_sat_interp_'+str(grid)+'.csv')\n",
    "U_sat_interp.to_csv(write_dir+'U_sat_interp_'+str(grid)+'.csv')\n",
    "V_sat_interp.to_csv(write_dir+'V_sat_interp_'+str(grid)+'.csv')\n",
    "##-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a616b-8b91-4eb1-b302-8dcea2644d67",
   "metadata": {},
   "source": [
    "## Script runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aec6b4",
   "metadata": {
    "title": "End timer"
   },
   "outputs": [],
   "source": [
    "runtime_end = timeit.default_timer() # stop the clock\n",
    "extraction_runtime = runtime_end-runtime_start\n",
    "print('\\nData Extraction Runtime:')\n",
    "print(str(round(extraction_runtime,5)),'secs')\n",
    "print(str(round((extraction_runtime)/60,5)),'mins')\n",
    "print(str(round((extraction_runtime)/3600,5)),'hrs')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
